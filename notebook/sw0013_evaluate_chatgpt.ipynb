{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25065b7e-c105-44a1-a3eb-fa68c6b690a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aphri\\miniconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fbd2792-7deb-4b07-81cc-adec5f6dfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(dir):\n",
    "    with open(dir, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "def write_pickle(dir, data):\n",
    "    with open(dir, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.t1 = None\n",
    "\n",
    "    @staticmethod\n",
    "    def delta_to_string(td):\n",
    "\n",
    "        res_list = []\n",
    "\n",
    "        def format():\n",
    "            return \", \".join(reversed(res_list)) + \" elapsed.\"\n",
    "\n",
    "        seconds = td % 60\n",
    "        td //= 60\n",
    "        res_list.append(f\"{round(seconds,3)} seconds\")\n",
    "\n",
    "        if td <= 0:\n",
    "            return format()\n",
    "\n",
    "        minutes = td % 60\n",
    "        td //= 60\n",
    "        res_list.append(f\"{minutes} minutes\")\n",
    "\n",
    "        if td <= 0:\n",
    "            return format()\n",
    "\n",
    "        hours = td % 24\n",
    "        td //= 24\n",
    "        res_list.append(f\"{hours} hours\")\n",
    "\n",
    "        if td <= 0:\n",
    "            return format()\n",
    "\n",
    "        res_list.append(f\"{td} days\")\n",
    "\n",
    "        return format()\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        self.t1 = time.time()\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "\n",
    "        t2 = time.time()\n",
    "        td = t2 - self.t1\n",
    "\n",
    "        print(self.delta_to_string(td))\n",
    "\n",
    "\n",
    "def top_n(input_dict, n):\n",
    "    return dict(sorted(input_dict.items(), key=itemgetter(1), reverse=True)[:n])\n",
    "\n",
    "\n",
    "def find_products(text_input, category_df, title_df):\n",
    "\n",
    "    text_input = [text_input]\n",
    "\n",
    "    # stage one, compare categories\n",
    "    category_df = category_df[~category_df[\"encoded_category\"].isna()]\n",
    "    categories = list(category_df[\"category\"].values)\n",
    "\n",
    "    categories_features = torch.stack(list(category_df[\"encoded_category\"].values))\n",
    "    encoded_texts = clip.tokenize(text_input).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        text_features = model.encode_text(encoded_texts)\n",
    "\n",
    "        categories_features /= categories_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity =  100 * categories_features @ text_features.T\n",
    "\n",
    "    res = dict(zip(categories, similarity.reshape(-1).tolist()))\n",
    "\n",
    "    res = sorted(res.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    n = 100\n",
    "    res_list = []\n",
    "    res_count = 0\n",
    "    for r in res:\n",
    "        cur_res = meta_df[meta_df[\"combined_category\"] == r[0]]\n",
    "        res_list.append(cur_res)\n",
    "        res_count += len(cur_res)\n",
    "        if res_count >= n:\n",
    "            break\n",
    "\n",
    "    # stage two, compare titles\n",
    "    res = pd.concat(res_list, axis=0)\n",
    "    res = res.title.values\n",
    "\n",
    "    title_df = title_df[title_df.title.isin(res)]\n",
    "    titles = list(title_df[\"title\"].values)\n",
    "\n",
    "    title_features = torch.stack(list(title_df[\"encoded_title\"].values))\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        title_features /= title_features.norm(dim=-1, keepdim=True)\n",
    "        similarity =  100 * title_features @ text_features.T\n",
    "\n",
    "    res = dict(zip(titles, similarity.reshape(-1).tolist()))\n",
    "    \n",
    "    res = sorted(res.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    n = 5\n",
    "    res = res[:n]\n",
    "    res_set = set([r[0] for r in res])\n",
    "    res = meta_df[meta_df[\"title\"].isin(res_set)][\"uniq_id\"].values\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def show_images(res):\n",
    "    n = len(res)\n",
    "    fig, ax = plt.subplots(1, n)\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5 * n)\n",
    "    \n",
    "    iterable = True\n",
    "    try:\n",
    "       it = ax[0]\n",
    "    except:\n",
    "        iterable = False\n",
    "\n",
    "    if not iterable:\n",
    "        img_path = image_path(res[0])\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        for i, image in enumerate(res):\n",
    "            img_path = image_path(image)\n",
    "            img = mpimg.imread(img_path)\n",
    "\n",
    "            ax[i].imshow(img)\n",
    "            ax[i].axis('off')\n",
    "            # ax[i].set_title(get_label(image), fontsize=8)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def image_path(uid):\n",
    "    return osp.join(image_storage, f\"{uid}.jpg\")\n",
    "\n",
    "\n",
    "def load_data(pickle_path, new_pickle_path):\n",
    "    category_df = read_pickle(osp.join(pickle_path, \"categories.pkl\"))\n",
    "    title_df = read_pickle(osp.join(pickle_path, \"titles.pkl\"))\n",
    "    meta_df = read_pickle(osp.join(pickle_path, \"meta_data.pkl\"))\n",
    "    meta_df_chatgpt = read_pickle(osp.join(new_pickle_path, \"meta_data_chatgpt.pkl\"))\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "    return device, model, preprocess, category_df, title_df, meta_df, meta_df_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8cda090-c5f1-4196-b4e0-946f8f2c8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"demo_data/eval_res1_chatgpt.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3db85d8-bd0f-4568-9352-bbb94ab9ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.923 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "image_storage = \"demo_data/target_images\"\n",
    "pickle_path = \"demo_data/pickle\"\n",
    "new_pickle_path = \"demo_data/data3_pickle\"\n",
    "\n",
    "with Timer():\n",
    "    (\n",
    "        device,\n",
    "        model, \n",
    "        preprocess,\n",
    "        category_df,\n",
    "        title_df,\n",
    "        meta_df,\n",
    "        meta_df_chatgpt\n",
    "    ) = load_data(pickle_path, new_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a37745ae-255b-4ba8-b29b-ad5c01dfd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_path = \"demo_data/product_description.pkl\"\n",
    "dataset_path = \"data/cleaned_target_furniture_dataset.csv\"\n",
    "\n",
    "if osp.exists(des_path):\n",
    "    d1 = read_pickle(des_path)\n",
    "else:\n",
    "    d1 = pd.read_csv(dataset_path)\n",
    "    d1 = d1[[\"uniq_id\", \"description\", \"sub_category_2\", \"colors\"]]\n",
    "    write_pickle(des_path, d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5c28c32-f5a2-4a12-b655-6a822fb5fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match(answer, res):\n",
    "    s1 = {tuple(v) for v in d1[d1.uniq_id.isin([answer])][[\"sub_category_2\", \"colors\"]].values}\n",
    "    s2 = {tuple(v) for v in d1[d1.uniq_id.isin(res)][[\"sub_category_2\", \"colors\"]].values}\n",
    "    \n",
    "    ans = len(s1.intersection(s2))\n",
    "    return ans > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "589a3cbe-1cd7-4159-b1ec-56e12a8cee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_total = 0\n",
    "curr_right = 0\n",
    "right_set = set()\n",
    "\n",
    "if osp.exists(eval_path):\n",
    "    res = read_pickle(eval_path)\n",
    "    curr_total = res[\"total\"]\n",
    "    curr_right = res[\"right\"]\n",
    "    right_set = res[\"right_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41ababee-5f5c-4f00-9f09-e1b8422f9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = meta_df_chatgpt\n",
    "count = 0\n",
    "for idx, row in df1.iterrows():\n",
    "    \n",
    "    count += 1\n",
    "    if count < curr_total + 1:\n",
    "        continue\n",
    "    \n",
    "    query = row[\"chatgpt_title\"]\n",
    "    answer = row[\"uniq_id\"]\n",
    "    res = find_products(query, category_df, title_df)\n",
    "    curr_total += 1\n",
    "    if is_match(answer, res):\n",
    "        curr_right += 1\n",
    "        right_set.add(answer)\n",
    "    \n",
    "    if curr_total % 200 == 0:\n",
    "        print(f\"{curr_total} current accuracy: {round(curr_right * 100/curr_total, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06ef9205-c442-4e4c-b344-e7df3f285fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict(\n",
    "    total=curr_total,\n",
    "    right=curr_right,\n",
    "    right_set=right_set\n",
    ")\n",
    "write_pickle(eval_path, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cee303d-8579-478e-90e8-bc2f52fc61c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(res[\"right\"] * 100 / res[\"total\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25795232-ba01-4a28-8107-9c1f8765a3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff58ee-ae58-40bb-8e7d-1fa8806c2ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
