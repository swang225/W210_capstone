{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8bd9bf2e-8429-4c57-b451-3cbde4e4727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcae221-822e-4600-b726-9db8a6d70264",
   "metadata": {},
   "source": [
    "### 1 save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eab2f0d1-761f-48ba-9f5b-12717464e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:/Users/aphri/Documents/t0002/work/data/w210_data/target_store_furniture_datasets.csv\"\n",
    "image_storage = \"C:/Users/aphri/Documents/t0002/work/data/w210_data/target_images\"\n",
    "pickle_path = \"C:/Users/aphri/Documents/t0002/work/data/w210_data/pickle\"\n",
    "model_path = \"C:/Users/aphri/Documents/t0002/work/data/w210_data/model\"\n",
    "\n",
    "Path(image_storage).mkdir(parents=True, exist_ok=True)\n",
    "Path(pickle_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25095e2a-5a82-4f8f-bc94-98d400c7c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ff5778-56c9-4889-b5f1-16f255e8b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_path(uid):\n",
    "    return osp.join(image_storage, f\"{uid}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0031be8f-3afe-4a1e-99b5-95facfa714d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prcoessed 2.37%\n",
      "prcoessed 4.74%\n",
      "prcoessed 7.11%\n",
      "prcoessed 9.48%\n",
      "prcoessed 11.84%\n",
      "prcoessed 14.21%\n",
      "prcoessed 16.58%\n",
      "prcoessed 18.95%\n",
      "prcoessed 21.32%\n",
      "prcoessed 23.69%\n",
      "prcoessed 26.06%\n",
      "prcoessed 28.43%\n",
      "prcoessed 30.79%\n",
      "prcoessed 33.16%\n",
      "prcoessed 35.53%\n",
      "prcoessed 37.9%\n",
      "prcoessed 40.27%\n",
      "prcoessed 42.64%\n",
      "prcoessed 45.01%\n",
      "prcoessed 47.38%\n",
      "prcoessed 49.75%\n",
      "prcoessed 52.11%\n",
      "prcoessed 54.48%\n",
      "prcoessed 56.85%\n",
      "prcoessed 59.22%\n",
      "prcoessed 61.59%\n",
      "prcoessed 63.96%\n",
      "prcoessed 66.33%\n",
      "prcoessed 68.7%\n",
      "prcoessed 71.06%\n",
      "prcoessed 73.43%\n",
      "prcoessed 75.8%\n",
      "prcoessed 78.17%\n",
      "prcoessed 80.54%\n",
      "prcoessed 82.91%\n",
      "prcoessed 85.28%\n",
      "prcoessed 87.65%\n",
      "prcoessed 90.02%\n",
      "prcoessed 92.38%\n",
      "prcoessed 94.75%\n",
      "prcoessed 97.12%\n",
      "prcoessed 99.49%\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = len(d1)\n",
    "for idx, v in d1[[\"main_image\", \"uniq_id\"]].iterrows():\n",
    "    url = v.main_image\n",
    "    uid = v.uniq_id\n",
    "    path = image_path(uid)\n",
    "    if not osp.exists(path):\n",
    "        image = Image.open(requests.get(url, stream=True).raw)\n",
    "        image.save(image_path(uid))\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"prcoessed {round(count/total * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ebedc-2a4e-4ff2-a921-6bc11df68ffc",
   "metadata": {},
   "source": [
    "### 2 process images and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b81f105-5e68-45ab-a0fa-ac308520f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(dir):\n",
    "    with open(dir, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "def write_pickle(dir, data):\n",
    "    with open(dir, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ec6decd-df17-4c7e-b29d-4e6cb6a38140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(name, uid_list, text_list, eimage_list, etext_list):\n",
    "    df = pd.DataFrame(data={\n",
    "        \"uid\": uid_list,\n",
    "        \"text\": text_list,\n",
    "        \"encoded_image\": eimage_list,\n",
    "        \"encoded_text\": etext_list\n",
    "    })\n",
    "    \n",
    "    write_pickle(name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6da3fa-3504-4b89-ba60-3655c9e63d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6302ca-ee11-49c3-894f-0be65888e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d1[[\"uniq_id\", \"sub_category_2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdba7965-abdb-4ffc-84f5-528f24d24f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prcoessed 2.37%\n",
      "prcoessed 4.74%\n",
      "prcoessed 7.11%\n",
      "prcoessed 9.48%\n",
      "prcoessed 11.84%\n",
      "prcoessed 14.21%\n",
      "prcoessed 16.58%\n",
      "prcoessed 18.95%\n",
      "prcoessed 21.32%\n",
      "prcoessed 23.69%\n",
      "prcoessed 26.06%\n",
      "prcoessed 28.43%\n",
      "prcoessed 30.79%\n",
      "prcoessed 33.16%\n",
      "prcoessed 35.53%\n",
      "prcoessed 37.9%\n",
      "prcoessed 40.27%\n",
      "prcoessed 42.64%\n",
      "prcoessed 45.01%\n",
      "prcoessed 47.38%\n",
      "prcoessed 49.75%\n",
      "prcoessed 52.11%\n",
      "prcoessed 54.48%\n",
      "prcoessed 56.85%\n",
      "prcoessed 59.22%\n",
      "prcoessed 61.59%\n",
      "prcoessed 63.96%\n",
      "prcoessed 66.33%\n",
      "prcoessed 68.7%\n",
      "prcoessed 71.06%\n",
      "prcoessed 73.43%\n",
      "prcoessed 75.8%\n",
      "prcoessed 78.17%\n",
      "prcoessed 80.54%\n",
      "prcoessed 82.91%\n",
      "prcoessed 85.28%\n",
      "prcoessed 87.65%\n",
      "prcoessed 90.02%\n",
      "prcoessed 92.38%\n",
      "prcoessed 94.75%\n",
      "prcoessed 97.12%\n",
      "prcoessed 99.49%\n"
     ]
    }
   ],
   "source": [
    "uid_list = []\n",
    "text_list = []\n",
    "eimage_list = []\n",
    "etext_list = []\n",
    "\n",
    "count = 0\n",
    "total = len(d1)\n",
    "for idx, row in d1.iterrows():\n",
    "    uid = row.uniq_id\n",
    "    text = row.sub_category_2\n",
    "    \n",
    "    uid_list.append(uid)\n",
    "    text_list.append(text)\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path(uid))).unsqueeze(0)\n",
    "        text = clip.tokenize(text)\n",
    "\n",
    "        eimage_list.append(image)\n",
    "        etext_list.append(text)\n",
    "    except:\n",
    "        # print(f\"failed: {uid}, {text}\")\n",
    "        eimage_list.append(None)\n",
    "        etext_list.append(None)\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"prcoessed {round(count/total * 100, 2)}%\")\n",
    "        save_processed_data(osp.join(pickle_path, f\"{count}.pkl\"), uid_list, text_list, eimage_list, etext_list)\n",
    "        uid_list = []\n",
    "        text_list = []\n",
    "        eimage_list = []\n",
    "        etext_list = []\n",
    "    \n",
    "if len(uid_list) > 0:\n",
    "    save_processed_data(osp.join(pickle_path, f\"{count}.pkl\"), uid_list, text_list, eimage_list, etext_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4161a-94fe-4db0-bcd1-384f4a3dc8f1",
   "metadata": {},
   "source": [
    "### 3 fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "149bc3d4-593a-4dec-b9a5-75ce256b9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(data, device):\n",
    "    encoded_images = torch.cat(list(data[\"encoded_image\"].values)).to(device)\n",
    "    encoded_texts = torch.cat(list(data[\"encoded_text\"].values)).to(device)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(encoded_images, encoded_texts)\n",
    "\n",
    "    # during training # images == # texts, so calc only once\n",
    "    n_classes = logits_per_image.shape[0]\n",
    "    device = images.device\n",
    "    labels = torch.arange(n_classes, device=device, dtype=torch.long)\n",
    "\n",
    "    loss_image = criterion(logits_per_image, labels)\n",
    "    loss_text = criterion(logits_per_text, labels)\n",
    "    curr_loss = (loss_image + loss_text) / 2\n",
    "    \n",
    "    return curr_loss\n",
    "\n",
    "\n",
    "def train(model, pickle_path, pickles, criterion, device, batch_size, optimizer, max_norm):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for p in pickles:\n",
    "        print(f\"training: {p}\")\n",
    "        file = osp.join(pickle_path, f\"{p}.pkl\")\n",
    "        data = read_pickle(file)\n",
    "        data = data[~data[\"encoded_text\"].isna()]\n",
    "        \n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < len(data):\n",
    "            sub_data = data[start:end]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            curr_loss = calc_loss(sub_data, device)\n",
    "            \n",
    "            curr_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += curr_loss.item()\n",
    "            total_count += len(sub_data)\n",
    "            \n",
    "            start = end\n",
    "            end += batch_size\n",
    "            sys.stdout.write(\".\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "            \n",
    "        print(f\"average loss: {total_loss/total_count}\")\n",
    "            \n",
    "            \n",
    "def evaluate(model, pickle_path, pickles, criterion, device, batch_size):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for p in pickles:\n",
    "        print(f\"evaluating: {p}\")\n",
    "        \n",
    "        file = osp.join(pickle_path, f\"{p}.pkl\")\n",
    "        data = read_pickle(file)\n",
    "        data = data[~data[\"encoded_text\"].isna()]\n",
    "        \n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < len(data):\n",
    "            sub_data = data[start:end]\n",
    "            curr_loss = calc_loss(sub_data, device)\n",
    "            \n",
    "            total_loss += curr_loss.item()\n",
    "            total_count += len(sub_data)\n",
    "            \n",
    "            start = end\n",
    "            end += batch_size\n",
    "            sys.stdout.write(\".\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "            \n",
    "        print(f\"average loss: {total_loss/total_count}\")\n",
    "    \n",
    "    \n",
    "def run_epoch(\n",
    "        model,\n",
    "        pickle_path,\n",
    "        train_pickles,\n",
    "        eval_pickles,\n",
    "        criterion,\n",
    "        device,\n",
    "        batch_size,\n",
    "        optimizer,\n",
    "        max_norm,\n",
    "        n_epoch,\n",
    "        seed=1234,\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    for n in range(n_epoch):\n",
    "        print(f\"training epoch: {n}\")\n",
    "        train(model, pickle_path, pickles, criterion, device, batch_size, optimizer, max_norm)\n",
    "        evaluate(model, pickle_path, pickles, criterion, device, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171b6b7-0694-4576-8c5e-cd462a36dcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 0\n",
      "training: 1000\n"
     ]
    }
   ],
   "source": [
    "train_pickles = [\n",
    "    1000, 2000, 3000, 4000, 5000, 6000, \n",
    "    7000, 8000, 9000, 10000, 11000, 12000, \n",
    "    13000, 14000, 15000, 16000, 17000, 18000, \n",
    "    19000, 20000, 21000, 22000, 23000, 24000, \n",
    "    25000, 26000, 27000, 28000, 29000, 30000, \n",
    "    31000, 32000, 33000, 34000, 35000, 36000, \n",
    "    37000, 38000, 39000 \n",
    "]\n",
    "eval_pickles = [40000, 41000, 42215]\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "max_norm = 1 # for gradient clipping\n",
    "n_epoch = 10\n",
    "\n",
    "run_epoch(model, pickle_path, train_pickles, eval_pickles, criterion, device, batch_size, optimizer, max_norm, n_epoch)\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    str(osp.join(model_path, 'model.pt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db5311-512f-47cf-a932-5994e53a13ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
