{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2ee38f-d6e7-40dd-8b34-ad9becdb58cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aphri\\miniconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfdb653-a5d6-4c74-aa42-dfbd18bce87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(dir):\n",
    "    with open(dir, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "def write_pickle(dir, data):\n",
    "    with open(dir, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.t1 = None\n",
    "\n",
    "    @staticmethod\n",
    "    def delta_to_string(td):\n",
    "\n",
    "        res_list = []\n",
    "\n",
    "        def format():\n",
    "            return \", \".join(reversed(res_list)) + \" elapsed.\"\n",
    "\n",
    "        seconds = td % 60\n",
    "        td //= 60\n",
    "        res_list.append(f\"{round(seconds,3)} seconds\")\n",
    "\n",
    "        if td <= 0:\n",
    "            return format()\n",
    "\n",
    "        minutes = td % 60\n",
    "        td //= 60\n",
    "        res_list.append(f\"{minutes} minutes\")\n",
    "\n",
    "        if td <= 0:\n",
    "            return format()\n",
    "\n",
    "        hours = td % 24\n",
    "        td //= 24\n",
    "        res_list.append(f\"{hours} hours\")\n",
    "\n",
    "        if td <= 0:\n",
    "            return format()\n",
    "\n",
    "        res_list.append(f\"{td} days\")\n",
    "\n",
    "        return format()\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        self.t1 = time.time()\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "\n",
    "        t2 = time.time()\n",
    "        td = t2 - self.t1\n",
    "\n",
    "        print(self.delta_to_string(td))\n",
    "\n",
    "\n",
    "def top_n(input_dict, n):\n",
    "    return dict(sorted(input_dict.items(), key=itemgetter(1), reverse=True)[:n])\n",
    "\n",
    "\n",
    "def find_products(text_input, category_df, image_pickle_path):\n",
    "\n",
    "    text_input = [text_input]\n",
    "\n",
    "    # stage one, compare categories\n",
    "    category_df = category_df[~category_df[\"encoded_category\"].isna()]\n",
    "    categories = list(category_df[\"category\"].values)\n",
    "\n",
    "    categories_features = torch.stack(list(category_df[\"encoded_category\"].values))\n",
    "    encoded_texts = clip.tokenize(text_input).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        text_features = model.encode_text(encoded_texts)\n",
    "\n",
    "        categories_features /= categories_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity =  100 * categories_features @ text_features.T\n",
    "\n",
    "    res = dict(zip(categories, similarity.reshape(-1).tolist()))\n",
    "\n",
    "    res = sorted(res.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    n = 10\n",
    "    res = res[:n]\n",
    "    res_set = set([r[0] for r in res])\n",
    "    \n",
    "    # do image matching\n",
    "    res = []\n",
    "    for cat in res_set:\n",
    "        store_path = osp.join(image_pickle_path, f\"{cat}.pkl\")\n",
    "        cat_res = read_pickle(store_path)\n",
    "        res.append(cat_res)\n",
    "    res = pd.concat(res, axis=0)\n",
    "    \n",
    "    uniq_ids = list(res[\"uid\"].values)\n",
    "    image_features = torch.stack(list(res[\"encoded_image\"].values))\n",
    "    similarity =  100 * image_features @ text_features.T\n",
    "    res = dict(zip(uniq_ids, similarity.reshape(-1).tolist()))\n",
    "    res = sorted(res.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    n = 5\n",
    "    res = res[:n]\n",
    "    res_set = set([r[0] for r in res])\n",
    "    \n",
    "    return res_set\n",
    "\n",
    "\n",
    "def show_images(res):\n",
    "    n = len(res)\n",
    "    fig, ax = plt.subplots(1, n)\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5 * n)\n",
    "    \n",
    "    iterable = True\n",
    "    try:\n",
    "       it = ax[0]\n",
    "    except:\n",
    "        iterable = False\n",
    "\n",
    "    if not iterable:\n",
    "        img_path = image_path(res[0])\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        for i, image in enumerate(res):\n",
    "            img_path = image_path(image)\n",
    "            img = mpimg.imread(img_path)\n",
    "\n",
    "            ax[i].imshow(img)\n",
    "            ax[i].axis('off')\n",
    "            # ax[i].set_title(get_label(image), fontsize=8)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def image_path(uid):\n",
    "    return osp.join(image_storage, f\"{uid}.jpg\")\n",
    "\n",
    "\n",
    "def load_data(pickle_path):\n",
    "    category_df = read_pickle(osp.join(pickle_path, \"categories.pkl\"))\n",
    "    meta_df = read_pickle(osp.join(pickle_path, \"meta_data.pkl\"))\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "    return device, model, preprocess, category_df, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d649bd89-3225-45bc-b1f1-701411bfd6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_category(x):\n",
    "    res = x[\"sub_category_1\"]\n",
    "    if not pd.isna(x[\"sub_category_2\"]):\n",
    "        res += \", \" + x[\"sub_category_2\"]\n",
    "    if not pd.isna(x[\"sub_category_3\"]):\n",
    "        res += \", \" + x[\"sub_category_3\"]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004a0cbb-c335-4b6a-aa1e-f0be0443ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(d1):\n",
    "    d1 = d1[d1[\"primary_category\"] == \"Furniture\"]\n",
    "    d1 = d1[~(d1[\"primary_category\"].isna() & d1[\"sub_category_1\"].isna() & d1[\"sub_category_2\"].isna() & d1[\"sub_category_3\"].isna())]\n",
    "    d1 = d1[~(d1[\"sub_category_1\"].isna() & d1[\"sub_category_2\"].isna() & d1[\"sub_category_3\"].isna())]\n",
    "    d1 = d1[~d1[\"description\"].isna()]\n",
    "    d1 = d1[~d1[\"colors\"].isna()]\n",
    "    d1[\"colors\"] = d1[\"colors\"].astype(str)\n",
    "    d1[\"combined_category\"] = d1.apply(combine_category, axis=1)\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc787671-1e3c-40d0-ac7d-19ef592e3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(x):\n",
    "    res = \"\"\n",
    "    if not pd.isna(x[\"colors\"]):\n",
    "        res += x[\"colors\"]\n",
    "    if not pd.isna(x[\"material\"]):\n",
    "        res += \" \" + x[\"material\"]\n",
    "    if not pd.isna(x[\"sub_category_2\"]):\n",
    "        res += \" \" + x[\"sub_category_2\"]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64b7d15a-84f3-4bf8-9fa4-8a8f2d9e2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match(answer, res):\n",
    "    s1 = {tuple(v) for v in d10[d10.uniq_id.isin([answer])][[\"sub_category_2\", \"colors\"]].values}\n",
    "    s2 = {tuple(v) for v in d10[d10.uniq_id.isin(res)][[\"sub_category_2\", \"colors\"]].values}\n",
    "    \n",
    "    ans = len(s1.intersection(s2))\n",
    "    return ans > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f0fcf9-a6a9-49df-859b-e67a5024667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_storage = \"demo_data/target_images\"\n",
    "pickle_path = \"demo_data/data3_pickle\"\n",
    "image_pickle_path = \"demo_data/data3_image_pickle\"\n",
    "dataset_path = \"data/cleaned_target_furniture_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32802472-dc39-4971-8a07-3c85f8179eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"demo_data/eval_res2_comb_no_finetune.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce13a0bb-5867-4416-888f-bbc9b8b20d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 minutes, 22.016 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    (\n",
    "        device,\n",
    "        model, \n",
    "        preprocess,\n",
    "        category_df,\n",
    "        meta_df\n",
    "    ) = load_data(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e02f76f-399c-49ae-b897-86bce1d9495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(dataset_path)\n",
    "d1 = clean_data(d1)\n",
    "d10 = d1[[\"uniq_id\", \"sub_category_2\", \"colors\"]]\n",
    "d1 = d1[d1[\"combined_category\"] == \"Home Office Furniture, Bookshelves & Bookcases\"]\n",
    "\n",
    "d1 = d1[[\"uniq_id\", \"sub_category_2\", \"material\", \"colors\"]]\n",
    "d1[\"comb\"] = d1.apply(comb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcac7346-cb1a-4556-b7ad-fc6f70b1c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_total = 0\n",
    "curr_right = 0\n",
    "right_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c95c96b2-fa7c-42fb-85c1-f1d68325ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 current accuracy: 72.5%\n",
      "400 current accuracy: 71.75%\n",
      "600 current accuracy: 74.5%\n",
      "800 current accuracy: 74.38%\n",
      "1000 current accuracy: 74.7%\n"
     ]
    }
   ],
   "source": [
    "df1 = d1\n",
    "count = 0\n",
    "for idx, row in df1.iterrows():\n",
    "    count += 1\n",
    "    if count < curr_total + 1:\n",
    "        continue\n",
    "    \n",
    "    query = row[\"comb\"]\n",
    "    answer = row[\"uniq_id\"]\n",
    "    res = find_products(query, category_df, image_pickle_path)\n",
    "    curr_total += 1\n",
    "    if is_match(answer, res):\n",
    "        curr_right += 1\n",
    "        right_set.add(answer)\n",
    "    \n",
    "    if curr_total % 200 == 0:\n",
    "        print(f\"{curr_total} current accuracy: {round(curr_right * 100/curr_total, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfbab3cf-5ff6-4e4e-b07f-7219a278f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict(\n",
    "    total=curr_total,\n",
    "    right=curr_right,\n",
    "    right_set=right_set\n",
    ")\n",
    "write_pickle(eval_path, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a8ec921-7238-41f2-ad56-14f0c00d54e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.71"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(res[\"right\"] * 100 / res[\"total\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9546752b-5db5-4117-9cbb-6415a4f1b103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa60526-7c9b-4abd-b1b0-0b9fdffcc00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
