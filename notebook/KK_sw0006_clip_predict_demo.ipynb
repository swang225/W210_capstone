{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swang225/W210_capstone/blob/main/notebook/KK_sw0006_clip_predict_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPNUglaVvbwZ",
        "outputId": "725f83ad-9fd1-47e6-edc6-0c09a53b1a93"
      },
      "id": "xPNUglaVvbwZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: clip in /usr/local/lib/python3.10/dist-packages (1.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip) (0.2.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip) (17.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import clip\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import requests\n",
        "import os.path as osp\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from operator import itemgetter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import time"
      ],
      "metadata": {
        "id": "AcpSCKg0vPd6"
      },
      "id": "AcpSCKg0vPd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95776e5-4311-4f36-8d0c-899bad82cd31",
      "metadata": {
        "id": "d95776e5-4311-4f36-8d0c-899bad82cd31"
      },
      "outputs": [],
      "source": [
        "class Timer:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.t1 = None\n",
        "\n",
        "    @staticmethod\n",
        "    def delta_to_string(td):\n",
        "\n",
        "        res_list = []\n",
        "\n",
        "        def format():\n",
        "            return \", \".join(reversed(res_list)) + \" elapsed.\"\n",
        "\n",
        "        seconds = td % 60\n",
        "        td //= 60\n",
        "        res_list.append(f\"{round(seconds,3)} seconds\")\n",
        "\n",
        "        if td <= 0:\n",
        "            return format()\n",
        "\n",
        "        minutes = td % 60\n",
        "        td //= 60\n",
        "        res_list.append(f\"{minutes} minutes\")\n",
        "\n",
        "        if td <= 0:\n",
        "            return format()\n",
        "\n",
        "        hours = td % 24\n",
        "        td //= 24\n",
        "        res_list.append(f\"{hours} hours\")\n",
        "\n",
        "        if td <= 0:\n",
        "            return format()\n",
        "\n",
        "        res_list.append(f\"{td} days\")\n",
        "\n",
        "        return format()\n",
        "\n",
        "    def __enter__(self):\n",
        "\n",
        "        self.t1 = time.time()\n",
        "\n",
        "    def __exit__(self, *args, **kwargs):\n",
        "\n",
        "        t2 = time.time()\n",
        "        td = t2 - self.t1\n",
        "\n",
        "        print(self.delta_to_string(td))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92dffcda-71e9-4250-a907-dfc691c9c562",
      "metadata": {
        "id": "92dffcda-71e9-4250-a907-dfc691c9c562"
      },
      "outputs": [],
      "source": [
        "def image_path(uid):\n",
        "    return osp.join(image_storage, f\"{uid}.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c769359a-7dc9-448c-bbfe-424533da1285",
      "metadata": {
        "id": "c769359a-7dc9-448c-bbfe-424533da1285"
      },
      "outputs": [],
      "source": [
        "def read_pickle(dir):\n",
        "    with open(dir, 'rb') as handle:\n",
        "        b = pickle.load(handle)\n",
        "    return b\n",
        "\n",
        "\n",
        "def write_pickle(dir, data):\n",
        "    with open(dir, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3096b2-52df-473c-af0d-5ba7847c15fd",
      "metadata": {
        "id": "ff3096b2-52df-473c-af0d-5ba7847c15fd"
      },
      "outputs": [],
      "source": [
        "def find_products(text_input, data):\n",
        "    print(f\"finding products for query: {text_input}...\")\n",
        "    text_input = [text_input]\n",
        "\n",
        "    data = data[~data[\"encoded_image\"].isna()]\n",
        "    image_uids = list(data[\"uid\"].values)\n",
        "\n",
        "    encoded_images = torch.cat(list(data[\"encoded_image\"].values)).to(device)\n",
        "    encoded_texts = clip.tokenize(text_input).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_per_image, logits_per_text = model(encoded_images, encoded_texts)\n",
        "        probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    res = dict(zip(image_uids, probs[0] * 100))\n",
        "    res = dict(sorted(res.items(), key=itemgetter(1), reverse=True)[:5])\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def show_images(res):\n",
        "    n = len(res)\n",
        "    fig, ax = plt.subplots(1, n)\n",
        "\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(5 * n)\n",
        "\n",
        "    for i, image in enumerate(res.keys()):\n",
        "        img_path = image_path(image)\n",
        "        img = mpimg.imread(img_path)\n",
        "        ax[i].imshow(img)\n",
        "        ax[i].axis('off')\n",
        "        # ax[i].set_title(get_label(image), fontsize=8)\n",
        "\n",
        "    plt.subplots_adjust(wspace=0, hspace=0.1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff44ff60-5693-44ed-b6e1-28232298ba05",
      "metadata": {
        "id": "ff44ff60-5693-44ed-b6e1-28232298ba05"
      },
      "outputs": [],
      "source": [
        "def save_processed_data(name, uid_list, eimage_list):\n",
        "    df = pd.DataFrame(data={\n",
        "        \"uid\": uid_list,\n",
        "        \"encoded_image\": eimage_list\n",
        "    })\n",
        "\n",
        "    write_pickle(name, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a7b1fc-fce7-4e69-808f-705e18158934",
      "metadata": {
        "id": "d5a7b1fc-fce7-4e69-808f-705e18158934"
      },
      "outputs": [],
      "source": [
        "image_storage = \"demo_data/image\"\n",
        "pickle_path = \"demo_data/pickle\"\n",
        "\n",
        "Path(image_storage).mkdir(parents=True, exist_ok=True)\n",
        "Path(pickle_path).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ddc3bcf-46cb-4ad4-851f-5b511958258d",
      "metadata": {
        "id": "5ddc3bcf-46cb-4ad4-851f-5b511958258d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bxTUQDvxVkg",
        "outputId": "b2196c26-2f5b-4c91-bc01-4109ce510db4"
      },
      "id": "8bxTUQDvxVkg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rchrlw5b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-rchrlw5b\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (17.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzLqp3JCyE_K",
        "outputId": "59482747-a0ba-4d66-c3b1-df1dda4ddcd8"
      },
      "id": "lzLqp3JCyE_K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-clip in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from openai-clip) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-clip) (4.66.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip) (0.2.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbfa037-12e7-48bf-9186-c2b4244b98cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "cdbfa037-12e7-48bf-9186-c2b4244b98cb",
        "outputId": "4fe1aaf2-d12a-4f7f-c789-44a00072d1cb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8a9b843a33d2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ffdb72-dc6c-4b36-ac8f-4608c03d72c4",
      "metadata": {
        "id": "36ffdb72-dc6c-4b36-ac8f-4608c03d72c4"
      },
      "outputs": [],
      "source": [
        "demo_pickle_path = osp.join(pickle_path, f\"demo.pkl\")\n",
        "\n",
        "if not osp.exists(demo_pickle_path):\n",
        "    print(\"demo pickle does not exist, converting demo images to demo pickle...\")\n",
        "\n",
        "    uid_list = []\n",
        "    eimage_list = []\n",
        "    for filename in os.listdir(image_storage):\n",
        "        f = os.path.join(image_storage, filename)\n",
        "        # checking if it is a file\n",
        "        if os.path.isfile(f):\n",
        "            uid = filename.split(\".\")[0]\n",
        "            uid_list.append(uid)\n",
        "\n",
        "            image = preprocess(Image.open(image_path(uid))).unsqueeze(0)\n",
        "            eimage_list.append(image)\n",
        "\n",
        "    save_processed_data(demo_pickle_path, uid_list, eimage_list)\n",
        "    print(\"done\")\n",
        "\n",
        "print(\"reading demo pickle\")\n",
        "data = read_pickle(demo_pickle_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64a90f6-45fc-44a0-b505-76f196f61ca8",
      "metadata": {
        "id": "c64a90f6-45fc-44a0-b505-76f196f61ca8"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925ef89f-ca32-4f60-98a2-aef9c282ea38",
      "metadata": {
        "id": "925ef89f-ca32-4f60-98a2-aef9c282ea38"
      },
      "outputs": [],
      "source": [
        "text_input = \"I am looking for a large beige office chair\"\n",
        "\n",
        "with Timer():\n",
        "    res1 = find_products(text_input, data)\n",
        "\n",
        "print(res1)\n",
        "show_images(res1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "578fa896-5709-4bff-833d-16930365bf9f",
      "metadata": {
        "id": "578fa896-5709-4bff-833d-16930365bf9f"
      },
      "source": [
        "### DEMO: ChatGPT + CLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "W5LXpV8gyj73"
      },
      "id": "W5LXpV8gyj73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150601bf-aed7-486e-bfed-4f606a7db09a",
      "metadata": {
        "id": "150601bf-aed7-486e-bfed-4f606a7db09a"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "#openai.api_key = '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'\n",
        "openai.api_key = 'key'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "res_list = []\n",
        "\n",
        "prefix = (\n",
        "    \"considering what the user asked before, what is the user looking for with the following request.\"\n",
        "    \" Only respond with the product description no more than 30 words:\"\n",
        ")\n",
        "while True:\n",
        "    message = input(\"User : \")\n",
        "    if message:\n",
        "        print(f\"User entered: {message}\")\n",
        "        messages.append(\n",
        "            {\"role\": \"user\", \"content\": f\"{prefix} {message}\"},\n",
        "        )\n",
        "        chat = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\", messages=messages\n",
        "        )\n",
        "\n",
        "        reply = chat.choices[0].message.content\n",
        "        print(f\"ChatGPT: {reply}\")\n",
        "\n",
        "        with Timer():\n",
        "            print(\"looking for products...\")\n",
        "            res_list.append(find_products(reply, data))\n",
        "            show_images(res_list[-1])\n",
        "            print(\"found products\")\n",
        "\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})"
      ],
      "metadata": {
        "id": "nQqD97w_FZsT"
      },
      "id": "nQqD97w_FZsT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c69cf34-bdf1-4416-86e3-345cd31bb780",
      "metadata": {
        "id": "6c69cf34-bdf1-4416-86e3-345cd31bb780"
      },
      "outputs": [],
      "source": [
        "# this demo uses 1000 products in the dataset\n",
        "\n",
        "# TODO\n",
        "# append product description shown to chatGPT for history reference\n",
        "# have chatGPT determine whether to search for image or ask for additional information\n",
        "# also ask chatGPT whethere the user has ended the conversation.\n",
        "# add  additional logic for situations such as: user complaining that none of the products match what was asked, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206c4525-70de-44c1-93f5-a86cc87c270d",
      "metadata": {
        "id": "206c4525-70de-44c1-93f5-a86cc87c270d"
      },
      "outputs": [],
      "source": [
        "# User input history:\n",
        "\n",
        "# I am looking for a large beige office chair\n",
        "# Actually I am looking for something darker\n",
        "# I don't need the wheels."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kisha Update below"
      ],
      "metadata": {
        "id": "6yWvt76TFGAM"
      },
      "id": "6yWvt76TFGAM"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "riWTspHknbse"
      },
      "id": "riWTspHknbse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-gpu"
      ],
      "metadata": {
        "id": "oWnfMWmloFiW"
      },
      "id": "oWnfMWmloFiW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "id": "MHbUhh-coGZJ"
      },
      "id": "MHbUhh-coGZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "id": "VEdrLmdWoJgX"
      },
      "id": "VEdrLmdWoJgX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import streamlit as st\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "#from dotenv import load_dotenv\n",
        "\n",
        "#load_dotenv()\n",
        "import io\n",
        "\n",
        "\n",
        "import openai\n",
        "\n",
        "import os\n"
      ],
      "metadata": {
        "id": "91UoNoLinaEe"
      },
      "id": "91UoNoLinaEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive, files\n",
        "import io\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "3sVFwdk9pS9p"
      },
      "id": "3sVFwdk9pS9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"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\"\n"
      ],
      "metadata": {
        "id": "-L8lD4TNpiA_"
      },
      "id": "-L8lD4TNpiA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - I am loading best practice examples of furniture description to train GPT for better output"
      ],
      "metadata": {
        "id": "RvOz6mbCF8wq"
      },
      "id": "RvOz6mbCF8wq"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load best practice examples\n",
        "#(later) can use textspliter for larger document\n",
        "#This is a sample best practice description & furniture style we are feeding into LLM. Action - look for sample data (we just need description, and furniture style column to feed in here)\n",
        "loadfile = CSVLoader(file_path='/content/drive/MyDrive/W210/Datasets/DescriptionExample.csv')\n",
        "data = loadfile.load()\n",
        "#print(data)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "#vetorizing and creating embedding using open source from Meta - FAISS\n",
        "db = FAISS.from_documents(data, embeddings)\n"
      ],
      "metadata": {
        "id": "uPeaY4M3oRho"
      },
      "id": "uPeaY4M3oRho",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Function for similarity search\n",
        "\n",
        "def retrieve_info(query):\n",
        "  #getting 3 top results that are similar\n",
        "    similar_response = db.similarity_search(query, k=3)\n",
        "\n",
        "    page_contents_array = [doc.page_content for doc in similar_response]\n",
        "\n",
        "    # print(page_contents_array)\n",
        "\n",
        "    return page_contents_array"
      ],
      "metadata": {
        "id": "P4jvhAkhFHIz"
      },
      "id": "P4jvhAkhFHIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Setup LLMChain & prompts so that GPT can generate more descriptive output than baseline gpt\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k-0613\")\n",
        "\n",
        "template=\"\"\"\n",
        "Interpret the user needs by understanding the input along with metadata.\n",
        "\n",
        "\n",
        "Below is the example of user needs:\n",
        "{description}\n",
        "\n",
        "Here is the list of metadata we normally need:\n",
        "{metadata}\n",
        "\n",
        "\n",
        "Please explain in one or two sentence what the user wants:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"description\", \"metadata\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "\n",
        "\n",
        "# 4. Retrieval augmented generation\n",
        "def generate_description_for_clip(description):\n",
        "   #step 1 - does similarity search\n",
        "    metadata = retrieve_info(description)\n",
        "\n",
        "    #step 2 - puts the similar best practice in the chain model\n",
        "    response=chain.run({'description': description,'metadata': metadata})\n",
        "    return response"
      ],
      "metadata": {
        "id": "HcfOnbSewuxO"
      },
      "id": "HcfOnbSewuxO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 - Updated so it asks back questions until all metadata is collected"
      ],
      "metadata": {
        "id": "GM8Pv-sgGeeA"
      },
      "id": "GM8Pv-sgGeeA"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "res_list = []\n",
        "\n",
        "prefix = (\n",
        "    \"considering what the user asked before, what is the user looking for with the following request.\"\n",
        "    \" Only respond with the product description no more than 30 words:\"\n",
        ")\n",
        "while True:\n",
        "    message = input(\"User : \")\n",
        "    if message:\n",
        "\n",
        "      print(f\"System msg - User entered: {message}\")\n",
        "      messages.append(\n",
        "            {\"role\": \"user\", \"content\": f\"{prefix} {message}\"},\n",
        "        )\n",
        "        # chat = openai.ChatCompletion.create(\n",
        "        #     model=\"gpt-3.5-turbo\", messages=messages\n",
        "        # )\n",
        "        # #reply=generate_description_for_clip(message)\n",
        "        # reply = chat.choices[0].message.content\n",
        "        # #reply=generate_description_for_clip(str(messages))\n",
        "        # print(f\"ChatGPT: {reply}\")\n",
        "\n",
        "    #if message:\n",
        "\n",
        "      needs=message\n",
        "      for i in ['color','price','material','room']:\n",
        "\n",
        "        messages2=[]\n",
        "        prefix_question =(\"Does this have information on \" +str(i)+ \" ?\" + \"Respond in yes or no\")\n",
        "\n",
        "        messages2.append(\n",
        "              {\"role\": \"user\", \"content\": f\"{prefix_question} {needs}\"},\n",
        "            )\n",
        "        chat = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\", messages=messages2\n",
        "                )\n",
        "        reply2 = chat.choices[0].message.content\n",
        "        print()\n",
        "        print(\"System msg - Did it have information on \" +str(i)+ \" ?\" , reply2)\n",
        "        if \"no\" in str.lower(reply2):\n",
        "\n",
        "          print(\"System msg - No info on \" +str(i) + \" was provided\")\n",
        "          data_asset=input(\"Is there a specific \"+ str(i) +\" you are looking for? :\")\n",
        "          needs=str(needs) + ' in ' +str(i) +' '+ str(data_asset)\n",
        "          print(\"System msg - \", needs)\n",
        "          messages.append(\n",
        "                    {\"role\": \"user\", \"content\": f\"{prefix} {needs}\"},\n",
        "                )\n",
        "        else:\n",
        "          print(\"System msg - Info on \" +str(i) + \" was provided\")\n",
        "          print(\"System msg - \", needs)\n",
        "          messages.append(\n",
        "                    {\"role\": \"user\", \"content\": f\"{prefix} {needs}\"},\n",
        "                )\n",
        "      # chat = openai.ChatCompletion.create(\n",
        "      #               model=\"gpt-3.5-turbo\", messages=messages2\n",
        "      #           )\n",
        "      #reply_base_chat = chat.choices[0].message.content\n",
        "\n",
        "      #using all info collected, generate description for clip, instead of baseline openai.ChatCompletion.create function\n",
        "      print()\n",
        "      print(\"System msg - FINAL \", needs)\n",
        "      reply=generate_description_for_clip(needs)\n",
        "\n",
        "\n",
        "      print(\"Description to put to Clip:\" , reply)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mU20H_AN3xya"
      },
      "id": "mU20H_AN3xya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Using one of reply to retrieve description that matches from our target data set"
      ],
      "metadata": {
        "id": "1s_XJ0kOGvRT"
      },
      "id": "1s_XJ0kOGvRT"
    },
    {
      "cell_type": "code",
      "source": [
        "reply"
      ],
      "metadata": {
        "id": "fDNTrbxl_Oo_"
      },
      "id": "fDNTrbxl_Oo_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute similarity score to match with description"
      ],
      "metadata": {
        "id": "suB98RPl_aUz"
      },
      "id": "suB98RPl_aUz"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sampleloadfile = CSVLoader(file_path='/content/drive/MyDrive/W210/Datasets/sub_target_store_furniture_datasets_sample.csv')\n",
        "sampledata = sampleloadfile.load()\n",
        "#print(data)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "#vetorizing and creating embedding using open source from Meta - FAISS\n",
        "sampledb = FAISS.from_documents(sampledata, embeddings)\n"
      ],
      "metadata": {
        "id": "1S6xlslD_emq"
      },
      "id": "1S6xlslD_emq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similar_search(query):\n",
        "  #getting 3 top results that are similar\n",
        "  similar_response = sampledb.similarity_search(query, k=3)\n",
        "  page_contents_array = [doc.page_content for doc in similar_response]\n",
        "\n",
        "  return page_contents_array"
      ],
      "metadata": {
        "id": "Mr_gR3gQAZJl"
      },
      "id": "Mr_gR3gQAZJl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_output=similar_search(reply)\n",
        "search_output"
      ],
      "metadata": {
        "id": "50DCiEVjA6p_"
      },
      "id": "50DCiEVjA6p_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 - Rouge score to evaluate the top output\n",
        "\n",
        "This can later be integrated with clip generated output\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. User inputs\n",
        "\n",
        "\n",
        "2. Llamaindex/knowledge graph powered GPT generates user needs\n",
        "\n",
        "    a. Feeds to Clip - Clip embedding similarity score\n",
        "    \n",
        "    b. Similarity score using description\n",
        "\n",
        "3. With i & ii , we use description vs user need and create rouge score\n",
        "\n",
        "4. Output image/url/description of top 5 items\n",
        "\n",
        "Conclusion: This way we are not only using clip for image search & but also using provided description to find items\n"
      ],
      "metadata": {
        "id": "4j7PlLnACrz3"
      },
      "id": "4j7PlLnACrz3"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "DYNXbyk_CtGT"
      },
      "id": "DYNXbyk_CtGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "\n",
        "def rouge(reply, search_output):\n",
        "  rank={}\n",
        "  reference = reply\n",
        "  for i in search_output:\n",
        "    hypothesis = i\n",
        "    print(\"Scores for \", i[0:100])\n",
        "\n",
        "  # Initialize the ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scores = scorer.score(reference, hypothesis)\n",
        "    #Used rouge L for now - can update later\n",
        "    evaluation_score=scores[\"rougeL\"].precision\n",
        "    rank[i]=evaluation_score\n",
        "\n",
        "    # Print the ROUGE scores\n",
        "    # print(\"ROUGE-1 Precision:\", scores[\"rouge1\"].precision)\n",
        "    # print(\"ROUGE-1 Recall:\", scores[\"rouge1\"].recall)\n",
        "    # print(\"ROUGE-1 F1 Score:\", scores[\"rouge1\"].fmeasure)\n",
        "\n",
        "    # print(\"ROUGE-2 Precision:\", scores[\"rouge2\"].precision)\n",
        "    # print(\"ROUGE-2 Recall:\", scores[\"rouge2\"].recall)\n",
        "    # print(\"ROUGE-2 F1 Score:\", scores[\"rouge2\"].fmeasure)\n",
        "\n",
        "    print(\"ROUGE-L Precision:\", scores[\"rougeL\"].precision)\n",
        "    print(\"ROUGE-L Recall:\", scores[\"rougeL\"].recall)\n",
        "    print(\"ROUGE-L F1 Score:\", scores[\"rougeL\"].fmeasure)\n",
        "    print()\n",
        "\n",
        "  max_key = max(rank, key=lambda k: rank[k])\n",
        "  print(\"Best match is item: \", max_key[0:100])\n"
      ],
      "metadata": {
        "id": "WJmK-FFbCv_P"
      },
      "id": "WJmK-FFbCv_P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge(reply, search_output)"
      ],
      "metadata": {
        "id": "P0xbnzaRERQ3"
      },
      "id": "P0xbnzaRERQ3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}